{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b509dbc1-3e67-442f-8cfc-83e252625f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell - additional imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Optional, Dict, List, Tuple, Set\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score\n",
    "import tqdm\n",
    "import random\n",
    "import os\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a04057-9394-4a90-b91b-a6feb914fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLAdClassifier(nn.Module):\n",
    "    \"\"\"Neural network that classifies each HTML token (or element start tag) as ad / non‑ad.\n",
    "\n",
    "    The model is intentionally conservative: a sigmoid output and a tunable probability\n",
    "    threshold allow you to bias toward *no* prediction rather than a false positive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        tag_vocab_size: int,\n",
    "        attr_vocab_size: int,\n",
    "        embed_dim: int = 256,\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 8,\n",
    "        dropout: float = 0.2,\n",
    "        max_seq_len: int = 1024,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # ──────────────────── Embedding blocks ────────────────────\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.tag_embed = nn.Embedding(tag_vocab_size, embed_dim, padding_idx=0)\n",
    "        self.attr_embed = nn.Embedding(attr_vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.embed_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ──────────────────── Transformer encoder ────────────────────\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # ──────────────────── Classification head ────────────────────\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 1),  # logit\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        token_ids: torch.LongTensor,      # (B, L)\n",
    "        tag_ids: torch.LongTensor,        # (B, L)\n",
    "        attr_ids: torch.LongTensor,       # (B, L)\n",
    "        pos_ids: torch.LongTensor,        # (B, L)\n",
    "        attention_mask: Optional[torch.BoolTensor] = None,  # (B, L)\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Return per‑token logits (before sigmoid).\"\"\"\n",
    "        x = (\n",
    "            self.token_embed(token_ids)\n",
    "            + self.tag_embed(tag_ids)\n",
    "            + self.attr_embed(attr_ids)\n",
    "            + self.pos_embed(pos_ids)\n",
    "        )\n",
    "        x = self.embed_dropout(x)\n",
    "\n",
    "        x = self.encoder(x, src_key_padding_mask=attention_mask)\n",
    "        logits = self.classifier(x).squeeze(-1)  # (B, L)\n",
    "        return logits\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Convenience helpers\n",
    "    # -------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def probability(logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert logits to probabilities with a numerically stable sigmoid.\"\"\"\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "    @staticmethod\n",
    "    def prediction(logits: torch.Tensor, threshold: float = 0.9) -> torch.Tensor:\n",
    "        \"\"\"Return boolean mask of predictions above threshold.\n",
    "        A high default threshold keeps false‑positives low.\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(logits) > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f25def4-9402-46e3-a14f-3d7b7ebb560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Tokenization and Processing\n",
    "\n",
    "class HTMLTokenizer:\n",
    "    \"\"\"Tokenizes HTML into components needed by the model.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len=1024):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.tag_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.attr_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.token_counter = Counter()\n",
    "        self.tag_counter = Counter()\n",
    "        self.attr_counter = Counter()\n",
    "        \n",
    "    def fit(self, html_documents: List[str], min_freq: int = 2):\n",
    "        \"\"\"Build vocabularies from a list of HTML documents.\"\"\"\n",
    "        for html in html_documents:\n",
    "            tokens, tags, attrs = self._tokenize_html(html)\n",
    "            self.token_counter.update(tokens)\n",
    "            self.tag_counter.update(tags)\n",
    "            self.attr_counter.update(attrs)\n",
    "            \n",
    "        # Build vocabularies, keeping only tokens that appear at least min_freq times\n",
    "        for token, count in self.token_counter.items():\n",
    "            if count >= min_freq and token not in self.token_vocab:\n",
    "                self.token_vocab[token] = len(self.token_vocab)\n",
    "                \n",
    "        for tag, count in self.tag_counter.items():\n",
    "            if count >= min_freq and tag not in self.tag_vocab:\n",
    "                self.tag_vocab[tag] = len(self.tag_vocab)\n",
    "                \n",
    "        for attr, count in self.attr_counter.items():\n",
    "            if count >= min_freq and attr not in self.attr_vocab:\n",
    "                self.attr_vocab[attr] = len(self.attr_vocab)\n",
    "                \n",
    "        print(f\"Vocab sizes: tokens={len(self.token_vocab)}, tags={len(self.tag_vocab)}, attrs={len(self.attr_vocab)}\")\n",
    "        \n",
    "    def _tokenize_html(self, html: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "        \"\"\"Extract tokens, tags, and attributes from HTML content.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        tokens, tags, attrs = [], [], []\n",
    "        \n",
    "        def process_node(node):\n",
    "            if isinstance(node, Tag):\n",
    "                # Process tag\n",
    "                tag_name = node.name.lower()\n",
    "                tags.append(tag_name)\n",
    "                tokens.append(f\"<{tag_name}>\")\n",
    "                attrs.append(\"tag_start\")\n",
    "                \n",
    "                # Process attributes\n",
    "                for attr, value in node.attrs.items():\n",
    "                    attr_text = attr.lower()\n",
    "                    tags.append(\"<UNK>\")  # Tags aren't relevant for attributes\n",
    "                    attrs.append(attr_text)\n",
    "                    \n",
    "                    if isinstance(value, list):\n",
    "                        value = \" \".join(value)\n",
    "                    elif not isinstance(value, str):\n",
    "                        value = str(value)\n",
    "                        \n",
    "                    tokens.append(value)\n",
    "                \n",
    "                # Process children\n",
    "                for child in node.children:\n",
    "                    process_node(child)\n",
    "                    \n",
    "                # Close tag\n",
    "                tags.append(tag_name)\n",
    "                tokens.append(f\"</{tag_name}>\")\n",
    "                attrs.append(\"tag_end\")\n",
    "            elif node.string and node.string.strip():\n",
    "                # Process text content\n",
    "                for word in re.findall(r'\\w+|[^\\w\\s]', node.string):\n",
    "                    tokens.append(word)\n",
    "                    tags.append(\"<UNK>\")\n",
    "                    attrs.append(\"<UNK>\")\n",
    "                    \n",
    "        process_node(soup)\n",
    "        return tokens, tags, attrs\n",
    "    \n",
    "    def encode(self, html: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Convert HTML to tensors suitable for the model.\"\"\"\n",
    "        tokens, tags, attrs = self._tokenize_html(html)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(tokens) > self.max_seq_len:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "            tags = tags[:self.max_seq_len]\n",
    "            attrs = attrs[:self.max_seq_len]\n",
    "            \n",
    "        # Convert to IDs\n",
    "        token_ids = [self.token_vocab.get(t, self.token_vocab[\"<UNK>\"]) for t in tokens]\n",
    "        tag_ids = [self.tag_vocab.get(t, self.tag_vocab[\"<UNK>\"]) for t in tags]\n",
    "        attr_ids = [self.attr_vocab.get(a, self.attr_vocab[\"<UNK>\"]) for a in attrs]\n",
    "        \n",
    "        # Create position IDs and attention mask\n",
    "        pos_ids = list(range(len(token_ids)))\n",
    "        attention_mask = [False] * len(token_ids)  # False = attend to this position\n",
    "        \n",
    "        # Pad if necessary\n",
    "        padding_length = self.max_seq_len - len(token_ids)\n",
    "        if padding_length > 0:\n",
    "            token_ids += [self.token_vocab[\"<PAD>\"]] * padding_length\n",
    "            tag_ids += [self.tag_vocab[\"<PAD>\"]] * padding_length\n",
    "            attr_ids += [self.attr_vocab[\"<PAD>\"]] * padding_length\n",
    "            pos_ids += [0] * padding_length  # Padding positions get 0\n",
    "            attention_mask += [True] * padding_length  # True = mask this position\n",
    "            \n",
    "        # Convert to tensors\n",
    "        return {\n",
    "            \"token_ids\": torch.tensor(token_ids, dtype=torch.long),\n",
    "            \"tag_ids\": torch.tensor(tag_ids, dtype=torch.long),\n",
    "            \"attr_ids\": torch.tensor(attr_ids, dtype=torch.long),\n",
    "            \"pos_ids\": torch.tensor(pos_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.bool)\n",
    "        }\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save tokenizer vocabularies.\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        torch.save({\n",
    "            \"token_vocab\": self.token_vocab,\n",
    "            \"tag_vocab\": self.tag_vocab,\n",
    "            \"attr_vocab\": self.attr_vocab,\n",
    "            \"max_seq_len\": self.max_seq_len\n",
    "        }, path)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"Load tokenizer from saved vocabularies.\"\"\"\n",
    "        data = torch.load(path)\n",
    "        tokenizer = cls(max_seq_len=data[\"max_seq_len\"])\n",
    "        tokenizer.token_vocab = data[\"token_vocab\"]\n",
    "        tokenizer.tag_vocab = data[\"tag_vocab\"]\n",
    "        tokenizer.attr_vocab = data[\"attr_vocab\"]\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b71ce6-acae-46a5-b749-e6c9ac29b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader classes\n",
    "\n",
    "class HTMLAdDataset(Dataset):\n",
    "    \"\"\"Dataset for HTML ad classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, html_documents: List[str], labels: List[List[int]], tokenizer: HTMLTokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            html_documents: List of HTML strings\n",
    "            labels: List of label lists (one label per token, 0=not ad, 1=ad)\n",
    "            tokenizer: HTMLTokenizer instance\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        for html, label_list in zip(html_documents, labels):\n",
    "            encoding = tokenizer.encode(html)\n",
    "            \n",
    "            # Ensure labels match the tokenized sequence length\n",
    "            seq_len = min(len(label_list), tokenizer.max_seq_len)\n",
    "            token_labels = label_list[:seq_len]\n",
    "            \n",
    "            # Pad labels if necessary\n",
    "            if len(token_labels) < tokenizer.max_seq_len:\n",
    "                token_labels = token_labels + [0] * (tokenizer.max_seq_len - len(token_labels))\n",
    "            \n",
    "            self.examples.append({\n",
    "                **encoding,\n",
    "                \"labels\": torch.tensor(token_labels, dtype=torch.float)\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def create_dataloaders(train_html, train_labels, val_html, val_labels, tokenizer, \n",
    "                       batch_size=8, oversample_ads=True):\n",
    "    \"\"\"Create training and validation DataLoaders with optional oversampling.\"\"\"\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = HTMLAdDataset(train_html, train_labels, tokenizer)\n",
    "    val_dataset = HTMLAdDataset(val_html, val_labels, tokenizer)\n",
    "    \n",
    "    # Identify examples with ads for oversampling\n",
    "    if oversample_ads:\n",
    "        ad_indices = []\n",
    "        for i, example in enumerate(train_dataset.examples):\n",
    "            if example[\"labels\"].sum() > 0:  # Contains at least one ad\n",
    "                ad_indices.append(i)\n",
    "        \n",
    "        # Create sampler that oversamples examples with ads\n",
    "        if ad_indices:\n",
    "            non_ad_indices = [i for i in range(len(train_dataset)) if i not in ad_indices]\n",
    "            weights = [5.0 if i in ad_indices else 1.0 for i in range(len(train_dataset))]\n",
    "            sampler = torch.utils.data.WeightedRandomSampler(\n",
    "                weights=weights,\n",
    "                num_samples=len(train_dataset),\n",
    "                replacement=True\n",
    "            )\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=batch_size,\n",
    "                sampler=sampler\n",
    "            )\n",
    "        else:\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a06dcf1-787e-4243-b143-cc9584eec0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, device=\"cuda\"):\n",
    "    \"\"\"Train the HTML ad classifier model.\"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Use weighted binary cross-entropy to handle class imbalance\n",
    "    # Weight positive examples (ads) much higher than negative examples\n",
    "    pos_weight = torch.tensor([10.0]).to(device)  # Adjust based on your dataset\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    best_val_auc = 0.0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_auc\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(\n",
    "                token_ids=batch[\"token_ids\"],\n",
    "                tag_ids=batch[\"tag_ids\"],\n",
    "                attr_ids=batch[\"attr_ids\"],\n",
    "                pos_ids=batch[\"pos_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "            \n",
    "            # Create a mask for non-padding tokens\n",
    "            non_pad_mask = ~batch[\"attention_mask\"]\n",
    "            \n",
    "            # Apply mask to logits and labels\n",
    "            masked_logits = logits[non_pad_mask]\n",
    "            masked_labels = batch[\"labels\"][non_pad_mask]\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(masked_logits, masked_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_auc, val_ap = evaluate_model(model, val_loader, criterion, device)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_auc\"].append(val_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, Val AP: {val_ap:.4f}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"val_auc\": val_auc\n",
    "            }, \"best_ad_classifier.pt\")\n",
    "            \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model on validation or test data.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(\n",
    "                token_ids=batch[\"token_ids\"],\n",
    "                tag_ids=batch[\"tag_ids\"],\n",
    "                attr_ids=batch[\"attr_ids\"],\n",
    "                pos_ids=batch[\"pos_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "            \n",
    "            # Create a mask for non-padding tokens\n",
    "            non_pad_mask = ~batch[\"attention_mask\"]\n",
    "            \n",
    "            # Apply mask to logits and labels\n",
    "            masked_logits = logits[non_pad_mask]\n",
    "            masked_labels = batch[\"labels\"][non_pad_mask]\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(masked_logits, masked_labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate probabilities\n",
    "            probs = torch.sigmoid(masked_logits)\n",
    "            \n",
    "            # Collect for metrics\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(masked_labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(dataloader)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # ROC-AUC score\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except:\n",
    "        auc = 0.5  # If only one class is present\n",
    "    \n",
    "    # Average precision score (PR-AUC)\n",
    "    try:\n",
    "        ap = average_precision_score(all_labels, all_probs)\n",
    "    except:\n",
    "        ap = 0.0\n",
    "        \n",
    "    return val_loss, auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6edc65e8-eb1d-470a-bceb-7fc6218bb9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection and preprocessing\n",
    "\n",
    "def fetch_html(url):\n",
    "    \"\"\"Fetch HTML content from a URL.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def identify_ad_elements(html, ad_patterns):\n",
    "    \"\"\"Identify ad elements in HTML based on patterns.\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tokens, tags, attrs = [], [], []\n",
    "    labels = []\n",
    "    \n",
    "    def is_ad_element(tag):\n",
    "        \"\"\"Check if an element is an ad based on patterns.\"\"\"\n",
    "        for pattern in ad_patterns:\n",
    "            # Check tag name\n",
    "            if pattern.get('tag') and pattern['tag'].lower() == tag.name.lower():\n",
    "                # Check attributes\n",
    "                for attr, value in pattern.get('attrs', {}).items():\n",
    "                    if attr in tag.attrs:\n",
    "                        tag_value = tag.attrs[attr]\n",
    "                        if isinstance(tag_value, list):\n",
    "                            tag_value = ' '.join(tag_value)\n",
    "                        elif not isinstance(tag_value, str):\n",
    "                            tag_value = str(tag_value)\n",
    "                            \n",
    "                        if isinstance(value, str) and value in tag_value:\n",
    "                            return True\n",
    "                        elif hasattr(value, 'search') and value.search(tag_value):  # Regex\n",
    "                            return True\n",
    "        return False\n",
    "    \n",
    "    def process_node(node, is_ad=False):\n",
    "        if isinstance(node, Tag):\n",
    "            # Check if this node is an ad\n",
    "            node_is_ad = is_ad or is_ad_element(node)\n",
    "            \n",
    "            # Process tag\n",
    "            tag_name = node.name.lower()\n",
    "            tags.append(tag_name)\n",
    "            tokens.append(f\"<{tag_name}>\")\n",
    "            attrs.append(\"tag_start\")\n",
    "            labels.append(1 if node_is_ad else 0)\n",
    "            \n",
    "            # Process attributes\n",
    "            for attr, value in node.attrs.items():\n",
    "                attr_text = attr.lower()\n",
    "                tags.append(\"<UNK>\")\n",
    "                attrs.append(attr_text)\n",
    "                \n",
    "                if isinstance(value, list):\n",
    "                    value = \" \".join(value)\n",
    "                elif not isinstance(value, str):\n",
    "                    value = str(value)\n",
    "                    \n",
    "                tokens.append(value)\n",
    "                labels.append(1 if node_is_ad else 0)\n",
    "            \n",
    "            # Process children\n",
    "            for child in node.children:\n",
    "                process_node(child, node_is_ad)\n",
    "                \n",
    "            # Close tag\n",
    "            tags.append(tag_name)\n",
    "            tokens.append(f\"</{tag_name}>\")\n",
    "            attrs.append(\"tag_end\")\n",
    "            labels.append(1 if node_is_ad else 0)\n",
    "        elif node.string and node.string.strip():\n",
    "            # Process text content\n",
    "            for word in re.findall(r'\\w+|[^\\w\\s]', node.string):\n",
    "                tokens.append(word)\n",
    "                tags.append(\"<UNK>\")\n",
    "                attrs.append(\"<UNK>\")\n",
    "                labels.append(1 if is_ad else 0)\n",
    "                \n",
    "    process_node(soup)\n",
    "    return tokens, tags, attrs, labels\n",
    "\n",
    "def collect_training_data(urls, ad_patterns):\n",
    "    \"\"\"Collect training data from a list of URLs.\"\"\"\n",
    "    html_documents = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for url in tqdm.tqdm(urls, desc=\"Collecting data\"):\n",
    "        html = fetch_html(url)\n",
    "        if html:\n",
    "            tokens, tags, attrs, labels = identify_ad_elements(html, ad_patterns)\n",
    "            html_documents.append(html)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return html_documents, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c447d057-a8d7-4a9a-8f86-99855e1e0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample ad patterns and URLs for training\n",
    "\n",
    "# Common ad patterns (add more based on your observations)\n",
    "ad_patterns = [\n",
    "    {'tag': 'div', 'attrs': {'class': re.compile(r'ad|advert|banner|promo', re.I)}},\n",
    "    {'tag': 'div', 'attrs': {'id': re.compile(r'ad|advert|banner|promo', re.I)}},\n",
    "    {'tag': 'iframe', 'attrs': {'src': re.compile(r'ad|advert|banner|doubleclick|googleads', re.I)}},\n",
    "    {'tag': 'a', 'attrs': {'href': re.compile(r'sponsored|ad\\.|advert|campaign\\?', re.I)}},\n",
    "    {'tag': 'div', 'attrs': {'data-ad': re.compile(r'.')}},  # any data-ad attribute\n",
    "    {'tag': 'script', 'attrs': {'src': re.compile(r'ad|pagead|adsbygoogle', re.I)}},\n",
    "    {'tag': 'ins', 'attrs': {'class': 'adsbygoogle'}},\n",
    "    {'tag': 'div', 'attrs': {'class': 'sponsored'}},\n",
    "    # Add patterns for specific sites you're targeting\n",
    "]\n",
    "\n",
    "# Example URLs to crawl (replace with actual URLs)\n",
    "urls = [\n",
    "    \"https://example.com\",\n",
    "    \"https://news.example.com\",\n",
    "    # Add more URLs\n",
    "]\n",
    "\n",
    "# For demonstration, let's create some synthetic data\n",
    "def create_synthetic_data(n_samples=100):\n",
    "    \"\"\"Create synthetic HTML samples with ads for testing.\"\"\"\n",
    "    html_samples = []\n",
    "    label_samples = []\n",
    "    \n",
    "    ad_classes = [\"ad-banner\", \"sponsored-content\", \"adsbygoogle\", \"promo-box\"]\n",
    "    ad_ids = [\"ad-container\", \"sponsored\", \"promotion\", \"adsense\"]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create random HTML structure\n",
    "        html = \"<html><body>\\n\"\n",
    "        \n",
    "        # Add header\n",
    "        html += \"<header><h1>Sample Page</h1></header>\\n\"\n",
    "        \n",
    "        # Add main content\n",
    "        html += \"<main>\\n\"\n",
    "        html += \"<article>\\n\"\n",
    "        html += \"<h2>Article Title</h2>\\n\"\n",
    "        html += \"<p>This is a sample paragraph with text content.</p>\\n\"\n",
    "        \n",
    "        # Add some sections\n",
    "        for j in range(random.randint(3, 6)):\n",
    "            html += f\"<section>\\n\"\n",
    "            html += f\"<h3>Section {j+1}</h3>\\n\"\n",
    "            html += \"<p>More content for this section with some text.</p>\\n\"\n",
    "            \n",
    "            # Maybe add an ad\n",
    "            if random.random() < 0.2:  # 20% chance of ad\n",
    "                ad_type = random.choice([\"class\", \"id\"])\n",
    "                if ad_type == \"class\":\n",
    "                    ad_value = random.choice(ad_classes)\n",
    "                    html += f'<div class=\"{ad_value}\">This is an advertisement!</div>\\n'\n",
    "                else:\n",
    "                    ad_value = random.choice(ad_ids)\n",
    "                    html += f'<div id=\"{ad_value}\">This is an advertisement!</div>\\n'\n",
    "                    \n",
    "            html += \"</section>\\n\"\n",
    "            \n",
    "        html += \"</article>\\n\"\n",
    "        html += \"</main>\\n\"\n",
    "        \n",
    "        # Add footer\n",
    "        html += \"<footer><p>Copyright example.com</p></footer>\\n\"\n",
    "        html += \"</body></html>\"\n",
    "        \n",
    "        tokens, tags, attrs, labels = identify_ad_elements(html, ad_patterns)\n",
    "        html_samples.append(html)\n",
    "        label_samples.append(labels)\n",
    "    \n",
    "    return html_samples, label_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31fbc187-a43e-45b9-a354-0c8b9d73dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab sizes: tokens=67, tags=15, attrs=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fungy\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/10: 100%|█████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.05it/s, loss=0.0677]\n",
      "C:\\Users\\fungy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.3850, Val Loss: 0.0355, Val AUC: 0.9972, Val AP: 0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 16.51it/s, loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 0.0547, Val Loss: 0.0343, Val AUC: 0.9970, Val AP: 0.8971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 16.90it/s, loss=0.00752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 0.0537, Val Loss: 0.0339, Val AUC: 0.9970, Val AP: 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 16.88it/s, loss=0.00584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 0.0451, Val Loss: 0.0349, Val AUC: 0.9971, Val AP: 0.9038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 16.88it/s, loss=0.00981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 0.0471, Val Loss: 0.0331, Val AUC: 0.9970, Val AP: 0.8945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 16.81it/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 0.0502, Val Loss: 0.0330, Val AUC: 0.9970, Val AP: 0.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 15.91it/s, loss=0.00976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Train Loss: 0.0355, Val Loss: 0.0333, Val AUC: 0.9969, Val AP: 0.8939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 16.63it/s, loss=0.00796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Train Loss: 0.0435, Val Loss: 0.0327, Val AUC: 0.9969, Val AP: 0.8942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 16.78it/s, loss=0.00838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Train Loss: 0.0385, Val Loss: 0.0327, Val AUC: 0.9970, Val AP: 0.8960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|████████████████████████████████████████████████████████| 63/63 [00:03<00:00, 16.88it/s, loss=0.0111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 0.0378, Val Loss: 0.0327, Val AUC: 0.9970, Val AP: 0.8977\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Save model and tokenizer\u001b[39;00m\n\u001b[0;32m     37\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml_ad_classifier.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml_tokenizer.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 121\u001b[0m, in \u001b[0;36mHTMLTokenizer.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save tokenizer vocabularies.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    122\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_vocab,\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_vocab,\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattr_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattr_vocab,\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_seq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len\n\u001b[0;32m    127\u001b[0m     }, path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: ''"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "\n",
    "# Create or load data\n",
    "train_html, train_labels = create_synthetic_data(500)  # For demo\n",
    "val_html, val_labels = create_synthetic_data(100)  # For demo\n",
    "\n",
    "# Build tokenizer and vocabularies\n",
    "tokenizer = HTMLTokenizer(max_seq_len=1024)\n",
    "tokenizer.fit(train_html + val_html)\n",
    "\n",
    "# Create model\n",
    "model = HTMLAdClassifier(\n",
    "    vocab_size=len(tokenizer.token_vocab),\n",
    "    tag_vocab_size=len(tokenizer.tag_vocab),\n",
    "    attr_vocab_size=len(tokenizer.attr_vocab),\n",
    "    embed_dim=256,\n",
    "    num_layers=4,\n",
    "    num_heads=8,\n",
    "    dropout=0.2,\n",
    "    max_seq_len=1024\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_html, train_labels, val_html, val_labels, tokenizer,\n",
    "    batch_size=8, oversample_ads=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, history = train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs=10, lr=1e-4, device=device\n",
    ")\n",
    "\n",
    "# Save model and tokenizer\n",
    "torch.save(model.state_dict(), \"html_ad_classifier.pt\")\n",
    "tokenizer.save(\"html_tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c163d6-54a7-4885-af67-c50c7e820723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference and visualization\n",
    "\n",
    "def detect_ads_in_html(html, model, tokenizer, threshold=0.9, device=\"cpu\"):\n",
    "    \"\"\"Detect ads in HTML and return annotated HTML.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tokens, tags, attrs = tokenizer._tokenize_html(html)\n",
    "    \n",
    "    # Encode for model\n",
    "    encoding = tokenizer.encode(html)\n",
    "    encoding = {k: v.unsqueeze(0).to(device) for k, v in encoding.items()}  # Add batch dimension\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoding)[0]  # Remove batch dimension\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    \n",
    "    # Process up to the actual content length\n",
    "    content_len = min(len(tokens), len(logits))\n",
    "    predictions = probs[:content_len] > threshold\n",
    "    \n",
    "    # Debug info\n",
    "    ad_elements = []\n",
    "    ad_token_ids = []\n",
    "    for i, (token, is_ad) in enumerate(zip(tokens[:content_len], predictions)):\n",
    "        if is_ad:\n",
    "            ad_token_ids.append(i)\n",
    "            if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "                tag_name = token[1:-1]\n",
    "                if not tag_name.startswith(\"/\"):  # Start tag, not end tag\n",
    "                    ad_elements.append(tag_name)\n",
    "    \n",
    "    # Now mark the ads in the parsed HTML\n",
    "    def mark_ad_elements(node, node_path=None, parent_is_ad=False):\n",
    "        if node_path is None:\n",
    "            node_path = []\n",
    "        \n",
    "        if isinstance(node, Tag):\n",
    "            # Check if this node corresponds to one of our ad elements\n",
    "            is_ad = parent_is_ad\n",
    "            \n",
    "            # Mark this node as an ad if it's in our list\n",
    "            if node.name in ad_elements:\n",
    "                is_ad = True\n",
    "                if not node.get(\"data-marked-as-ad\"):\n",
    "                    node[\"data-marked-as-ad\"] = \"true\"\n",
    "                    node[\"style\"] = \"border: 3px solid red; background-color: rgba(255, 0, 0, 0.1);\"\n",
    "            \n",
    "            # Process all children\n",
    "            for child in node.children:\n",
    "                mark_ad_elements(child, node_path + [node], is_ad)\n",
    "                \n",
    "    mark_ad_elements(soup)\n",
    "    \n",
    "    # Count the ads found\n",
    "    ad_count = len([1 for tag in soup.find_all() if tag.get(\"data-marked-as-ad\") == \"true\"])\n",
    "    \n",
    "    return {\n",
    "        \"annotated_html\": str(soup),\n",
    "        \"ad_count\": ad_count,\n",
    "        \"ad_elements\": ad_elements,\n",
    "        \"ad_token_ids\": ad_token_ids\n",
    "    }\n",
    "\n",
    "def visualize_training_history(history):\n",
    "    \"\"\"Visualize training and validation metrics.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss Curve\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history[\"val_auc\"], label=\"Validation AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Validation AUC\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "def test_on_real_website(url, model, tokenizer, threshold=0.9):\n",
    "    \"\"\"Test the model on a real website.\"\"\"\n",
    "    print(f\"Testing on {url}\")\n",
    "    html = fetch_html(url)\n",
    "    if not html:\n",
    "        print(\"Failed to fetch HTML\")\n",
    "        return\n",
    "        \n",
    "    result = detect_ads_in_html(html, model, tokenizer, threshold=threshold)\n",
    "    print(f\"Found {result['ad_count']} potential ads\")\n",
    "    print(f\"Ad element types: {result['ad_elements']}\")\n",
    "    \n",
    "    # Save annotated HTML for inspection\n",
    "    with open(\"annotated_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result[\"annotated_html\"])\n",
    "    print(\"Saved annotated HTML to annotated_page.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037b93b-2857-4ba1-8e6c-38e438883939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
